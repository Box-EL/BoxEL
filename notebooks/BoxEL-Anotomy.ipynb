{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "import wandb\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import math\n",
    "import argparse\n",
    "import os\n",
    "import json\n",
    "from utils import *\n",
    "from dataset import *\n",
    "from softbox import SoftBox\n",
    "from gumbel_box import GumbelBox\n",
    "\n",
    "# box_model = {'softbox': SoftBox,\n",
    "#              'gumbel': GumbelBox}\n",
    "\n",
    "torch.cuda.set_device(1)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "# data parameters\n",
    "parser.add_argument('--train_data_path', type=str, default='./data/full_wordnet/full_wordnet_noneg.tsv', help='path to train data')\n",
    "parser.add_argument('--test_data_path', type=str, default='./data/full_wordnet/full_wordnet.tsv', help='path to test data')\n",
    "parser.add_argument('--vocab_path', type=str, default='./data/full_wordnet/full_wordnet_vocab.tsv', help='path to vocab')\n",
    "# training parameters\n",
    "parser.add_argument('--no_cuda', action='store_true', default=False, help='disables CUDA training (eg. no nvidia GPU)')\n",
    "parser.add_argument('--random_negative_sampling_ratio', type=int, default=1, help='sample this many random negatives for each positive.')\n",
    "parser.add_argument('--log_batch_size', type=int, default=13, help='batch size for training will be 2**LOG_BATCH_SIZE')\n",
    "parser.add_argument('--learning_rate', type=float, default=5e-3, help='learning rate')\n",
    "parser.add_argument('--epochs', type=int, default=10000, help='number of epochs to train')\n",
    "parser.add_argument('--prediction_thres', type=float, default=0.8, help='the probability threshold for prediction')\n",
    "# model parameters\n",
    "parser.add_argument('--model', type=str, default='softbox', help='model type: choose from softbox, gumbel')\n",
    "parser.add_argument('--box_embedding_dim', type=int, default=40, help='box embedding dimension')\n",
    "parser.add_argument('--softplus_temp', type=float, default=1.0, help='beta of softplus function')\n",
    "# gumbel box parameters\n",
    "parser.add_argument('--gumbel_beta', type=float, default=1.0, help='beta value for gumbel distribution')\n",
    "parser.add_argument('--scale', type=float, default=1.0, help='scale value for gumbel distribution')\n",
    "# a parameter can be set to a model checkpoint path or left as None\n",
    "# If set, the checkpoint will be resumed and tested; the user needs to specify test_data_path and vocab_path but not others\n",
    "# Other parameters will be restored from the model checkpoint\n",
    "parser.add_argument('--resume_and_test', type=str, default=None, help='path to a model checkpoint to be resumed and tested')\n",
    "\n",
    "args = parser.parse_args(args=['--no_cuda'] )\n",
    "args.save_to = \"./checkpoints/\" + args.model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "eps = 1e-8\n",
    "\n",
    "def l2_side_regularizer(box, log_scale: bool = True):\n",
    "    \"\"\"Applies l2 regularization on all sides of all boxes and returns the sum.\n",
    "    \"\"\"\n",
    "    min_x = box.min_embed \n",
    "    delta_x = box.delta_embed  \n",
    "\n",
    "    if not log_scale:\n",
    "        return torch.mean(delta_x ** 2)\n",
    "    else:\n",
    "        return  torch.mean(F.relu(min_x + delta_x - 1 + eps )) + F.relu(torch.norm(min_x, p=2)-1)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import wandb\n",
    "import torch.nn as nn\n",
    "from basic_box import Box\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import uniform\n",
    "\n",
    "euler_gamma = 0.57721566490153286060\n",
    "\n",
    "gamma = 1\n",
    "\n",
    "class BoxEL(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, relation_size,embed_dim, min_init_value, delta_init_value, relation_init_value, scaling_init_value, args):\n",
    "        super(BoxEL, self).__init__()\n",
    "        min_embedding = self.init_concept_embedding(vocab_size, embed_dim, min_init_value)\n",
    "        delta_embedding = self.init_concept_embedding(vocab_size, embed_dim, delta_init_value)\n",
    "        relation_embedding = self.init_concept_embedding(relation_size, embed_dim, relation_init_value)\n",
    "        scaling_embedding = self.init_concept_embedding(relation_size, embed_dim, scaling_init_value)\n",
    "        \n",
    "        self.temperature = args.softplus_temp\n",
    "        self.min_embedding = nn.Parameter(min_embedding)\n",
    "        self.delta_embedding = nn.Parameter(delta_embedding)\n",
    "        self.relation_embedding = nn.Parameter(relation_embedding)\n",
    "        self.scaling_embedding = nn.Parameter(scaling_embedding)\n",
    "        \n",
    "        self.gumbel_beta = args.gumbel_beta\n",
    "        self.scale = args.scale\n",
    "\n",
    "    def forward(self, data):\n",
    "        nf1_min = self.min_embedding[data[0][:,[0,2]]]\n",
    "        nf1_delta = self.delta_embedding[data[0][:,[0,2]]]\n",
    "        nf1_max = nf1_min+torch.exp(nf1_delta)\n",
    "        \n",
    "        boxes1 = Box(nf1_min[:, 0, :], nf1_max[:, 0, :])\n",
    "        boxes2 = Box(nf1_min[:, 1, :], nf1_max[:, 1, :])\n",
    "        \n",
    "        nf1_loss, nf1_reg_loss = self.nf1_loss(boxes1, boxes2)\n",
    "        \n",
    "        nf2_min = self.min_embedding[data[1]]\n",
    "        nf2_delta = self.delta_embedding[data[1]]\n",
    "        nf2_max = nf2_min+torch.exp(nf2_delta)\n",
    "        \n",
    "        boxes1 = Box(nf2_min[:, 0, :], nf2_max[:, 0, :])\n",
    "        boxes2 = Box(nf2_min[:, 1, :], nf2_max[:, 1, :])\n",
    "        boxes3 = Box(nf2_min[:, 2, :], nf2_max[:, 2, :])\n",
    "        \n",
    "        nf2_loss,nf2_reg_loss = self.nf2_loss(boxes1, boxes2, boxes3)\n",
    "        \n",
    "        nf3_min = self.min_embedding[data[2][:,[0,2]]]\n",
    "        nf3_delta = self.delta_embedding[data[2][:,[0,2]]]\n",
    "        nf3_max = nf3_min+torch.exp(nf3_delta)\n",
    "        relation = self.relation_embedding[data[2][:,1]]\n",
    "        scaling = self.scaling_embedding[data[2][:,1]]\n",
    "        \n",
    "        boxes1 = Box(nf3_min[:, 0, :], nf3_max[:, 0, :])\n",
    "        boxes2 = Box(nf3_min[:, 1, :], nf3_max[:, 1, :])\n",
    "        \n",
    "        nf3_loss,nf3_reg_loss = self.nf3_loss(boxes1, relation, scaling, boxes2)\n",
    "        \n",
    "        nf4_min = self.min_embedding[data[3][:,1:]]\n",
    "        nf4_delta = self.delta_embedding[data[3][:,1:]]\n",
    "        nf4_max = nf4_min+torch.exp(nf4_delta)\n",
    "        relation = self.relation_embedding[data[3][:,0]]\n",
    "        scaling = self.scaling_embedding[data[3][:,0]]\n",
    "        \n",
    "        boxes1 = Box(nf4_min[:, 0, :], nf4_max[:, 0, :])\n",
    "        boxes2 = Box(nf4_min[:, 1, :], nf4_max[:, 1, :])\n",
    "        \n",
    "        nf4_loss,nf4_reg_loss = self.nf4_loss(relation, scaling, boxes1, boxes2)\n",
    "        \n",
    "        disjoint_min = self.min_embedding[data[4]]\n",
    "        disjoint_delta = self.delta_embedding[data[4]]\n",
    "        disjoint_max = disjoint_min+torch.exp(disjoint_delta)\n",
    "        boxes1 = Box(disjoint_min[:, 0, :], disjoint_max[:, 0, :])\n",
    "        boxes2 = Box(disjoint_min[:, 1, :], disjoint_max[:, 1, :])\n",
    "        disjoint_loss,disjoint_reg_loss = self.disjoint_loss(boxes1, boxes2)\n",
    "        \n",
    "        nf3_neg_min = self.min_embedding[data[6][:,[0,2]]]\n",
    "        nf3_neg_delta = self.delta_embedding[data[6][:,[0,2]]]\n",
    "        nf3_neg_max = nf3_neg_min+torch.exp(nf3_neg_delta)\n",
    "        \n",
    "        relation = self.relation_embedding[data[6][:,1]]\n",
    "        scaling = self.scaling_embedding[data[6][:,1]]\n",
    "        \n",
    "        boxes1 = Box(nf3_neg_min[:, 0, :], nf3_neg_max[:, 0, :])\n",
    "        boxes2 = Box(nf3_neg_min[:, 1, :], nf3_neg_max[:, 1, :])\n",
    "        \n",
    "        nf3_neg_loss,nf3_neg_reg_loss = self.nf3_neg_loss(boxes1, relation, scaling, boxes2)\n",
    "        \n",
    "        all_min = self.min_embedding\n",
    "        all_delta = self.delta_embedding\n",
    "        all_max = all_min+torch.exp(all_delta)\n",
    "        boxes = Box(all_min, all_max)\n",
    "        reg_loss = l2_side_regularizer(boxes, log_scale=True)\n",
    "        \n",
    "        return nf1_loss.sum(), nf2_loss.sum(), nf3_loss.sum(), nf4_loss.sum(), disjoint_loss.sum(), nf3_neg_loss.sum(), nf1_reg_loss, nf2_reg_loss , nf3_reg_loss , nf4_reg_loss , disjoint_reg_loss , nf3_neg_reg_loss\n",
    "\n",
    "    def get_cond_probs(self, data):\n",
    "        nf3_min = self.min_embedding[data[:,[0,2]]]\n",
    "        nf3_delta = self.delta_embedding[data[:,[0,2]]]\n",
    "        nf3_max = nf3_min+torch.exp(nf3_delta)\n",
    "        \n",
    "        relation = self.relation_embedding[data[:,1]]\n",
    "        \n",
    "        boxes1 = Box(nf3_min[:, 0, :], nf3_max[:, 0, :])\n",
    "        boxes2 = Box(nf3_min[:, 1, :], nf3_max[:, 1, :])\n",
    "        \n",
    "        log_intersection = torch.log(torch.clamp(self.volumes(self.intersection(boxes1, boxes2)), 1e-10, 1e4))\n",
    "        log_box2 = torch.log(torch.clamp(self.volumes(boxes2), 1e-10, 1e4))\n",
    "        return torch.exp(log_intersection-log_box2)\n",
    "        \n",
    "\n",
    "    def volumes(self, boxes):\n",
    "        return F.softplus(boxes.delta_embed, beta=self.temperature).prod(1)\n",
    "\n",
    "    def intersection(self, boxes1, boxes2):\n",
    "        intersections_min = torch.max(boxes1.min_embed, boxes2.min_embed)\n",
    "        intersections_max = torch.min(boxes1.max_embed, boxes2.max_embed)\n",
    "        intersection_box = Box(intersections_min, intersections_max)\n",
    "        return intersection_box\n",
    "    \n",
    "    def inclusion_loss(self, boxes1, boxes2):\n",
    "        log_intersection = torch.log(torch.clamp(self.volumes(self.intersection(boxes1, boxes2)), 1e-10, 1e4))\n",
    "        log_box1 = torch.log(torch.clamp(self.volumes(boxes1), 1e-10, 1e4))\n",
    "        \n",
    "        return 1-torch.exp(log_intersection-log_box1)\n",
    "    \n",
    "    def nf1_loss(self, boxes1, boxes2):\n",
    "        return self.inclusion_loss(boxes1, boxes2), l2_side_regularizer(boxes1, log_scale=True) + l2_side_regularizer(boxes2, log_scale=True)\n",
    "        \n",
    "    def nf2_loss(self, boxes1, boxes2, boxes3):\n",
    "        inter_box = self.intersection(boxes1, boxes2)\n",
    "        return self.inclusion_loss(inter_box, boxes3), l2_side_regularizer(inter_box, log_scale=True) + l2_side_regularizer(boxes1, log_scale=True) + l2_side_regularizer(boxes2, log_scale=True) + l2_side_regularizer(boxes3, log_scale=True)\n",
    "    \n",
    "    def nf3_loss(self, boxes1, relation, scaling, boxes2):\n",
    "        trans_min = boxes1.min_embed*(scaling + eps) + relation\n",
    "        trans_max = boxes1.max_embed*(scaling + eps) + relation\n",
    "        trans_boxes = Box(trans_min, trans_max)\n",
    "        return self.inclusion_loss(trans_boxes, boxes2), l2_side_regularizer(trans_boxes, log_scale=True) + l2_side_regularizer(boxes1, log_scale=True) + l2_side_regularizer(boxes2, log_scale=True) \n",
    "    \n",
    "    def nf4_loss(self, relation, scaling, boxes1, boxes2):\n",
    "        trans_min = (boxes1.min_embed - relation)/(scaling + eps)\n",
    "        trans_max = (boxes1.max_embed - relation)/(scaling + eps)\n",
    "        trans_boxes = Box(trans_min, trans_max)\n",
    "#         log_trans_boxes = torch.log(torch.clamp(self.volumes(trans_boxes), 1e-10, 1e4))\n",
    "        return self.inclusion_loss(trans_boxes, boxes2), l2_side_regularizer(trans_boxes, log_scale=True) + l2_side_regularizer(boxes1, log_scale=True) + l2_side_regularizer(boxes2, log_scale=True) \n",
    "        \n",
    "    def disjoint_loss(self, boxes1, boxes2):\n",
    "        log_intersection = torch.log(torch.clamp(self.volumes(self.intersection(boxes1, boxes2)), 1e-10, 1e4))\n",
    "        log_boxes1 = torch.log(torch.clamp(self.volumes(boxes1), 1e-10, 1e4))\n",
    "        log_boxes2 = torch.log(torch.clamp(self.volumes(boxes2), 1e-10, 1e4))\n",
    "        union = log_boxes1 + log_boxes2\n",
    "        return torch.exp(log_intersection-union), l2_side_regularizer(boxes1, log_scale=True) + l2_side_regularizer(boxes2, log_scale=True)\n",
    "        \n",
    "    def nf3_neg_loss(self, boxes1, relation, scaling, boxes2):\n",
    "        trans_min = boxes1.min_embed*(scaling + eps) + relation\n",
    "        trans_max = boxes1.max_embed*(scaling + eps) + relation\n",
    "        trans_boxes = Box(trans_min, trans_max)\n",
    "#         trans_min = boxes1.min_embed + relation\n",
    "#         trans_max = trans_min + torch.clamp((boxes1.max_embed - boxes1.min_embed)*(scaling + eps), 1e-10, 1e4)\n",
    "#         trans_boxes = Box(trans_min, trans_max)\n",
    "        return 1-self.inclusion_loss(trans_boxes, boxes2),l2_side_regularizer(trans_boxes, log_scale=True) + l2_side_regularizer(boxes1, log_scale=True) + l2_side_regularizer(boxes2, log_scale=True) \n",
    "        \n",
    "    def init_concept_embedding(self, vocab_size, embed_dim, init_value):\n",
    "        distribution = uniform.Uniform(init_value[0], init_value[1])\n",
    "        box_embed = distribution.sample((vocab_size, embed_dim))\n",
    "        return box_embed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"  \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "import click as ck\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "from scipy.stats import rankdata\n",
    "\n",
    "def load_valid_data(valid_data_file, classes, relations):\n",
    "    data = []\n",
    "    rel = f'SubClassOf'\n",
    "    with open(valid_data_file, 'r') as f:\n",
    "        for line in f:\n",
    "            it = line.strip().split()\n",
    "            id1 = it[0]\n",
    "            id2 = it[1]\n",
    "            if id1 not in classes or id2 not in classes or rel not in relations:\n",
    "                continue\n",
    "            data.append((classes[id1], relations[rel], classes[id2]))\n",
    "    return data\n",
    "\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "\n",
    "def load_cls(train_data_file):\n",
    "    train_subs=list()\n",
    "    counter=0\n",
    "    with open(train_data_file,'r') as f:\n",
    "        for line in f:\n",
    "            counter+=1\n",
    "            it = line.strip().split()\n",
    "            cls1 = it[0]\n",
    "            cls2 = it[1]\n",
    "            train_subs.append(cls1)\n",
    "            train_subs.append(cls2)\n",
    "    train_cls = list(set(train_subs))\n",
    "    return train_cls,counter\n",
    "\n",
    "\n",
    "#Original Loss\n",
    "def load_data(filename):\n",
    "    classes = {}\n",
    "    relations = {}\n",
    "    data = {'nf1': [], 'nf2': [], 'nf3': [], 'nf4': [], 'disjoint': []}\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            # Ignore SubObjectPropertyOf\n",
    "            if line.startswith('SubObjectPropertyOf'):\n",
    "                continue\n",
    "            # Ignore SubClassOf()\n",
    "            line = line.strip()[11:-1]\n",
    "            if not line:\n",
    "                continue\n",
    "            if line.startswith('ObjectIntersectionOf('):\n",
    "                # C and D SubClassOf E\n",
    "                it = line.split(' ')\n",
    "                c = it[0][21:]\n",
    "                d = it[1][:-1]\n",
    "                e = it[2]\n",
    "                if c not in classes:\n",
    "                    classes[c] = len(classes)\n",
    "                if d not in classes:\n",
    "                    classes[d] = len(classes)\n",
    "                if e not in classes:\n",
    "                    classes[e] = len(classes)\n",
    "                form = 'nf2'\n",
    "                if e == 'owl:Nothing':\n",
    "                    form = 'disjoint'\n",
    "                data[form].append((classes[c], classes[d], classes[e]))\n",
    "                \n",
    "            elif line.startswith('ObjectSomeValuesFrom('):\n",
    "                # R some C SubClassOf D\n",
    "                it = line.split(' ')\n",
    "                r = it[0][21:]\n",
    "                c = it[1][:-1]\n",
    "                d = it[2]\n",
    "                if c not in classes:\n",
    "                    classes[c] = len(classes)\n",
    "                if d not in classes:\n",
    "                    classes[d] = len(classes)\n",
    "                if r not in relations:\n",
    "                    relations[r] = len(relations)\n",
    "                data['nf4'].append((relations[r], classes[c], classes[d]))\n",
    "            elif line.find('ObjectSomeValuesFrom') != -1:\n",
    "                # C SubClassOf R some D\n",
    "                it = line.split(' ')\n",
    "                c = it[0]\n",
    "                r = it[1][21:]\n",
    "                d = it[2][:-1]\n",
    "                if c not in classes:\n",
    "                    classes[c] = len(classes)\n",
    "                if d not in classes:\n",
    "                    classes[d] = len(classes)\n",
    "                if r not in relations:\n",
    "                    relations[r] = len(relations)\n",
    "                data['nf3'].append((classes[c], relations[r], classes[d]))\n",
    "            else:\n",
    "                # C SubClassOf D\n",
    "                it = line.split(' ')\n",
    "                c = it[0]\n",
    "                d = it[1]\n",
    "                r = 'SubClassOf'\n",
    "                if r not in relations:\n",
    "                    relations[r] = len(relations)\n",
    "                if c not in classes:\n",
    "                    classes[c] = len(classes)\n",
    "                if d not in classes:\n",
    "                    classes[d] = len(classes)\n",
    "                data['nf1'].append((classes[c],relations[r],classes[d]))\n",
    "                \n",
    "    # Check if TOP in classes and insert if it is not there\n",
    "    if 'owl:Thing' not in classes:\n",
    "        classes['owl:Thing'] = len(classes)\n",
    "#changing by adding sub classes of train_data ids to prot_ids\n",
    "    prot_ids = []\n",
    "    class_keys = list(classes.keys())\n",
    "    for val in all_subcls:\n",
    "        if val not in class_keys:\n",
    "            cid = len(classes)\n",
    "            classes[val] = cid\n",
    "            prot_ids.append(cid)\n",
    "        else:\n",
    "            prot_ids.append(classes[val])\n",
    "\n",
    "    prot_ids = np.array(prot_ids)\n",
    "    \n",
    "    \n",
    "    # Add corrupted triples nf3\n",
    "    n_classes = len(classes)\n",
    "    data['nf3_neg'] = []\n",
    "    for c, r, d in data['nf3']:\n",
    "        x = np.random.choice(prot_ids)\n",
    "        while x == c:\n",
    "            x = np.random.choice(prot_ids)\n",
    "            \n",
    "        y = np.random.choice(prot_ids)\n",
    "        while y == d:\n",
    "             y = np.random.choice(prot_ids)\n",
    "        data['nf3_neg'].append((c, r,x))\n",
    "        data['nf3_neg'].append((y, r, d))\n",
    "        \n",
    "    \n",
    "    data['nf1'] = np.array(data['nf1'])\n",
    "    data['nf2'] = np.array(data['nf2'])\n",
    "    data['nf3'] = np.array(data['nf3'])\n",
    "    data['nf4'] = np.array(data['nf4'])\n",
    "    data['disjoint'] = np.array(data['disjoint'])\n",
    "    data['top'] = np.array([classes['owl:Thing'],])\n",
    "    data['nf3_neg'] = np.array(data['nf3_neg'])\n",
    "                            \n",
    "    for key, val in data.items():\n",
    "        index = np.arange(len(data[key]))\n",
    "        np.random.seed(seed=100)\n",
    "        np.random.shuffle(index)\n",
    "        data[key] = val[index]\n",
    "    \n",
    "    return data, classes, relations\n",
    "\n",
    "\n",
    "class Generator(object):\n",
    "    def __init__(self, data, batch_size=128, steps=100):\n",
    "        self.data = data\n",
    "        self.batch_size = batch_size\n",
    "        self.steps = steps\n",
    "        self.start = 0\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        return self.next()\n",
    "\n",
    "    def reset(self):\n",
    "        self.start = 0\n",
    "\n",
    "    def next(self):\n",
    "        if self.start < self.steps:\n",
    "            nf1_index = np.random.choice(\n",
    "                self.data['nf1'].shape[0], self.batch_size)\n",
    "            nf2_index = np.random.choice(\n",
    "                self.data['nf2'].shape[0], self.batch_size)\n",
    "            nf3_index = np.random.choice(\n",
    "                self.data['nf3'].shape[0], self.batch_size)\n",
    "            nf4_index = np.random.choice(\n",
    "                self.data['nf4'].shape[0], self.batch_size)\n",
    "            dis_index = np.random.choice(\n",
    "                self.data['disjoint'].shape[0], self.batch_size)\n",
    "            top_index = np.random.choice(\n",
    "                self.data['top'].shape[0], self.batch_size)\n",
    "            nf3_neg_index = np.random.choice(\n",
    "                self.data['nf3_neg'].shape[0], self.batch_size)\n",
    "            nf1 = self.data['nf1'][nf1_index]\n",
    "            nf2 = self.data['nf2'][nf2_index]\n",
    "            nf3 = self.data['nf3'][nf3_index]\n",
    "            nf4 = self.data['nf4'][nf4_index]\n",
    "            dis = self.data['disjoint'][dis_index]\n",
    "            top = self.data['top'][top_index]\n",
    "            nf3_neg = self.data['nf3_neg'][nf3_neg_index]\n",
    "            labels = np.zeros((self.batch_size, 1), dtype=np.float32)\n",
    "            self.start += 1\n",
    "            return ([nf1, nf2, nf3, nf4, dis, top, nf3_neg], labels)\n",
    "        else:\n",
    "            self.reset()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data samples: 85246\n",
      "Training data classes: 82533\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset = 'ANATOMY'\n",
    "\n",
    "total_sub_cls=[]\n",
    "train_file = f\"data/{dataset}/{dataset}_train.txt\"\n",
    "va_file = f\"data/{dataset}/{dataset}_valid.txt\"\n",
    "test_file = f\"data/{dataset}/{dataset}_test.txt\"\n",
    "train_sub_cls,train_samples = load_cls(train_file)\n",
    "valid_sub_cls,valid_samples = load_cls(va_file)\n",
    "test_sub_cls,test_samples = load_cls(test_file)\n",
    "total_sub_cls = train_sub_cls + valid_sub_cls + test_sub_cls\n",
    "all_subcls = list(set(total_sub_cls))\n",
    "\n",
    "print(\"Training data samples:\",train_samples)\n",
    "print(\"Training data classes:\",len(train_sub_cls))\n",
    "\n",
    "gdata_file=f\"data/{dataset}/{dataset}_latest_norm_mod.owl\"\n",
    "train_data, classes, relations = load_data(gdata_file)\n",
    "valid_data_file=f\"data/{dataset}/{dataset}_valid.txt\"\n",
    "valid_data = load_valid_data(valid_data_file, classes, relations)\n",
    "valid_data = torch.Tensor(valid_data).long().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no. classes: 106363\n",
      "no. relations: 157\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([106363, 1])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 256 \n",
    "\n",
    "proteins = {} # substitute for classes with subclass case\n",
    "for val in all_subcls:\n",
    "    proteins[val] = classes[val]\n",
    "nb_classes = len(classes)\n",
    "nb_relations = len(relations)\n",
    "\n",
    "print(\"no. classes:\",nb_classes)\n",
    "print(\"no. relations:\",nb_relations)\n",
    "nb_train_data = 0\n",
    "\n",
    "for key, val in train_data.items():\n",
    "    nb_train_data = max(len(val), nb_train_data)\n",
    "train_steps = int(math.ceil(nb_train_data / (1.0 * batch_size)))\n",
    "train_generator = Generator(train_data, batch_size, steps=train_steps)\n",
    "\n",
    "cls_dict = {v: k for k, v in classes.items()}\n",
    "rel_dict = {v: k for k, v in relations.items()}\n",
    "\n",
    "cls_list = []\n",
    "rel_list = []\n",
    "for i in range(nb_classes):\n",
    "    cls_list.append(cls_dict[i])\n",
    "for i in range(nb_relations):\n",
    "    rel_list.append(rel_dict[i])\n",
    "\n",
    "classes_index = list(classes.values())\n",
    "classes_index = torch.Tensor(classes_index).to(device).reshape(-1,1).long()\n",
    "classes_index.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3bvxslwk) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 60934<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>/workspace/geometric/pytorch_box/wandb/run-20210817_122218-3bvxslwk/logs/debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>/workspace/geometric/pytorch_box/wandb/run-20210817_122218-3bvxslwk/logs/debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>train loss</td><td>9.93052</td></tr><tr><td>nf1_loss</td><td>1.73843</td></tr><tr><td>nf2_loss</td><td>0.02561</td></tr><tr><td>nf3_loss</td><td>0.9397</td></tr><tr><td>nf4_loss</td><td>0.0004</td></tr><tr><td>nf3_neg_loss</td><td>3.81301</td></tr><tr><td>mean_rank</td><td>6570.69</td></tr><tr><td>valid_accuracy</td><td>0.77171</td></tr><tr><td>_runtime</td><td>13727</td></tr><tr><td>_timestamp</td><td>1629216665</td></tr><tr><td>_step</td><td>188</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>train loss</td><td>█▅▄▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>nf1_loss</td><td>█▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>nf2_loss</td><td>▁▆█▇▆▅▃▃▃▂▃▂▃▂▁▂▃▂▃▃▃▄▃▃▂▃▃▄▃▃▄▄▃▃▄▄▄▄▅▄</td></tr><tr><td>nf3_loss</td><td>█▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>nf4_loss</td><td>▂▅█▃▃▂▁▂▁▄▁▂▂▄▃▂▃▂▃▂▂▃▂▁▂▄▄▂▃▄▄▄▃▃▂▄▃▄▃▃</td></tr><tr><td>nf3_neg_loss</td><td>█▅▄▄▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>mean_rank</td><td>█▆▆▄▄▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂</td></tr><tr><td>valid_accuracy</td><td>▇██▇▆▆▅▅▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>_runtime</td><td>▁▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>_timestamp</td><td>▁▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">fragrant-universe-47</strong>: <a href=\"https://wandb.ai/boxiong/basic_box/runs/3bvxslwk\" target=\"_blank\">https://wandb.ai/boxiong/basic_box/runs/3bvxslwk</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "...Successfully finished last run (ID:3bvxslwk). Initializing new run:<br/><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: wandb version 0.12.0 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.11.0<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">absurd-valley-56</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/boxiong/basic_box\" target=\"_blank\">https://wandb.ai/boxiong/basic_box</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/boxiong/basic_box/runs/3d4db5bu\" target=\"_blank\">https://wandb.ai/boxiong/basic_box/runs/3d4db5bu</a><br/>\n",
       "                Run data is saved locally in <code>/workspace/geometric/pytorch_box/wandb/run-20210818_123550-3d4db5bu</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h1>Run(3d4db5bu)</h1><iframe src=\"https://wandb.ai/boxiong/basic_box/runs/3d4db5bu\" style=\"border:none;width:100%;height:400px\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f96feb014a8>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=\"basic_box\", reinit=True, config=args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model = BoxEL(nb_classes, nb_relations, 100, [1e-4, 0.2], [-0.1, 0], [-0.1,0.1], [0.9,1.1], args).to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "wandb.watch(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1 Train loss: 495.050903 Valid Mean Rank: 53182.0\n",
      "\n",
      "Epoch:101 Train loss: 73.576905 Valid Mean Rank: 43591.365\n",
      "\n",
      "Epoch:201 Train loss: 31.967320 Valid Mean Rank: 25118.95\n",
      "\n",
      "Epoch:301 Train loss: 20.635300 Valid Mean Rank: 9549.46\n",
      "\n",
      "Epoch:401 Train loss: 14.291499 Valid Mean Rank: 6265.985\n",
      "\n",
      "Epoch:501 Train loss: 11.020786 Valid Mean Rank: 5135.175\n",
      "\n",
      "Epoch:601 Train loss: 9.272273 Valid Mean Rank: 4630.465\n",
      "\n",
      "Epoch:701 Train loss: 8.239963 Valid Mean Rank: 4032.915\n",
      "\n",
      "Epoch:801 Train loss: 7.591425 Valid Mean Rank: 3757.335\n",
      "\n",
      "Epoch:901 Train loss: 7.162210 Valid Mean Rank: 3806.79\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Network error resolved after 0:00:44.544562, resuming normal operation.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.train()\n",
    "steps_per_epoch = train_steps\n",
    "train_generator.reset()\n",
    "\n",
    "epochs = 1000\n",
    "for epoch in range(epochs):\n",
    "    train_loss = 0.0\n",
    "    nf1_total_loss = 0.0\n",
    "    nf2_total_loss = 0.0\n",
    "    nf3_total_loss = 0.0\n",
    "    nf4_total_loss = 0.0\n",
    "    disjoint_total_loss = 0.0\n",
    "    nf3_neg_total_loss = 0.0\n",
    "    \n",
    "    for step, batch in enumerate(train_generator):\n",
    "        if step < steps_per_epoch:\n",
    "            nf1_loss, nf2_loss, nf3_loss, nf4_loss, disjoint_loss, nf3_neg_loss, nf1_reg_loss, nf2_reg_loss , nf3_reg_loss , nf4_reg_loss , disjoint_reg_loss , nf3_neg_reg_loss = model(batch[0])\n",
    "            loss =  nf1_loss + nf1_reg_loss + nf2_loss + nf2_reg_loss + 10*disjoint_loss + disjoint_reg_loss + nf3_loss + nf3_reg_loss + nf4_loss + nf4_reg_loss + nf3_neg_loss + nf3_neg_reg_loss\n",
    "            assert torch.isnan(loss).sum() == 0\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            assert torch.isnan(model.min_embedding).sum() == 0\n",
    "            optimizer.step()\n",
    "            assert torch.isnan(model.min_embedding).sum() == 0\n",
    "            assert torch.isnan(model.min_embedding.grad).sum() == 0\n",
    "            train_loss += loss\n",
    "            nf1_total_loss += nf1_loss\n",
    "            nf2_total_loss += nf2_loss\n",
    "            nf3_total_loss += nf3_loss\n",
    "            nf4_total_loss += nf4_loss\n",
    "            disjoint_total_loss += disjoint_loss\n",
    "            nf3_neg_total_loss += nf3_neg_loss\n",
    "        else:\n",
    "            train_generator.reset()\n",
    "            break\n",
    "            \n",
    "    mean_rank = compute_mean_rank(model, valid_data[0:100])\n",
    "    valid_accuracy = compute_accuracy(model, valid_data)\n",
    "    \n",
    "    wandb.log({'train loss': train_loss.item()/(step+1),'nf1_loss':nf1_total_loss.item()/(step+1), 'nf2_loss':nf2_total_loss.item()/(step+1), 'nf3_loss':nf3_total_loss.item()/(step+1), 'nf4_loss':nf4_total_loss.item()/(step+1), 'nf3_neg_loss':nf3_neg_total_loss.item()/(step+1), 'mean_rank':mean_rank,'valid_accuracy':valid_accuracy })\n",
    "\n",
    "    PATH = './models/box_el_anatomy_100.pt'\n",
    "    if epoch % 100 == 0:\n",
    "        torch.save(model, PATH)\n",
    "        print('Epoch:%d' %(epoch + 1), \"Train loss: %f\" %(train_loss.item()/(step+1)), f'Valid Mean Rank: {mean_rank}\\n')\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from scipy.stats import rankdata\n",
    "import statistics\n",
    "from collections import Counter\n",
    "\n",
    "def compute_cond_probs(model, boxes1, boxes2):\n",
    "    log_intersection = torch.log(torch.clamp(model.volumes(model.intersection(boxes1, boxes2)), 1e-10, 1e4))\n",
    "    log_box2 = torch.log(torch.clamp(model.volumes(boxes2), 1e-10, 1e4))\n",
    "    return torch.exp(log_intersection-log_box2)\n",
    "\n",
    "def compute_mean_rank(model, valid_data):\n",
    "    mean_rank = 0.0\n",
    "    n = len(valid_data)\n",
    "    for i, (c, r, d) in enumerate(valid_data):\n",
    "        c_data = torch.cat((c.repeat(classes_index.shape[0], 1), torch.Tensor([0]).repeat(classes_index.shape[0], 1).to(device).long(), classes_index), 1) \n",
    "        nf1_min = model.min_embedding[c_data[:,[0,2]]]\n",
    "        nf1_delta = model.delta_embedding[c_data[:,[0,2]]]\n",
    "        nf1_max = nf1_min+torch.exp(nf1_delta)\n",
    "        boxes1 = Box(nf1_min[:, 0, :], nf1_max[:, 0, :])\n",
    "        boxes2 = Box(nf1_min[:, 1, :], nf1_max[:, 1, :])\n",
    "        c_probs = 1- compute_cond_probs(model, boxes1, boxes2).cpu().detach().numpy()\n",
    "        index = rankdata(c_probs, method='average')\n",
    "        rank = index[d]\n",
    "        mean_rank += rank \n",
    "    mean_rank /= n\n",
    "    return mean_rank\n",
    "\n",
    "def compute_rank(model, valid_data, ratio):\n",
    "    rank_values = []\n",
    "    top1 = 0\n",
    "    top10 = 0\n",
    "    top100 = 0\n",
    "    n = len(valid_data)\n",
    "    rank_percentile = []\n",
    "    for i, (c, r, d) in enumerate(valid_data):\n",
    "        c_data = torch.cat((c.repeat(classes_index.shape[0], 1), torch.Tensor([0]).repeat(classes_index.shape[0], 1).to(device).long(), classes_index), 1) \n",
    "        nf1_min = model.min_embedding[c_data[:,[0,2]]]\n",
    "        nf1_delta = model.delta_embedding[c_data[:,[0,2]]]\n",
    "        nf1_max = nf1_min+torch.exp(nf1_delta)\n",
    "        boxes1 = Box(nf1_min[:, 0, :], nf1_max[:, 0, :])\n",
    "        boxes2 = Box(nf1_min[:, 1, :], nf1_max[:, 1, :])\n",
    "        c_probs = 1- compute_cond_probs(model, boxes1, boxes2).cpu().detach().numpy()\n",
    "        index = rankdata(c_probs, method='average')\n",
    "        rank = index[d]\n",
    "        rank_values.append(rank)\n",
    "        rank_percentile.append(rank)\n",
    "        if rank == 1:\n",
    "            top1 += 1\n",
    "        if rank <= 10:\n",
    "            top10 += 1\n",
    "        if rank <= 100:\n",
    "            top100 += 1\n",
    "    \n",
    "    top1 /= (i+1)\n",
    "    top10 /= (i+1)\n",
    "    top100 /= (i+1)\n",
    "    \n",
    "    mean_rank = np.mean(rank_values)\n",
    "    median_rank = statistics.median(rank_values)\n",
    "    rank_percentile.sort()\n",
    "    per_rank = np.percentile(rank_percentile,ratio)\n",
    "    rank_dicts = dict(Counter(rank_values))\n",
    "    nb_classes = model.min_embedding.shape[0]\n",
    "    auc = compute_rank_roc(rank_dicts,nb_classes)\n",
    "    return top1, top10, top100, mean_rank, median_rank, per_rank, auc, rank_values\n",
    "\n",
    "def compute_rank_roc(ranks, n):\n",
    "    auc_lst = list(ranks.keys())\n",
    "    auc_x = auc_lst[1:]\n",
    "    auc_x.sort()\n",
    "    auc_y = []\n",
    "    tpr = 0\n",
    "    sum_rank = sum(ranks.values())\n",
    "    for x in auc_x:\n",
    "        tpr += ranks[x]\n",
    "        auc_y.append(tpr / sum_rank)\n",
    "    auc_x.append(n)\n",
    "    auc_y.append(1)\n",
    "    auc = np.trapz(auc_y, auc_x)/n\n",
    "    return auc\n",
    "\n",
    "def compute_accuracy(model, test_data):\n",
    "    nf1_min = model.min_embedding[test_data[:,[0,2]]]\n",
    "    nf1_delta = model.delta_embedding[test_data[:,[0,2]]]\n",
    "    nf1_max = nf1_min+torch.exp(nf1_delta)\n",
    "    boxes1 = Box(nf1_min[:, 0, :], nf1_max[:, 0, :])\n",
    "    boxes2 = Box(nf1_min[:, 1, :], nf1_max[:, 1, :])\n",
    "    probs = compute_cond_probs(model, boxes1, boxes2).cpu().detach().numpy()\n",
    "    return np.sum(probs==1)/probs.shape[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_file = f'data/{dataset}/{dataset}_test.txt'\n",
    "test_data = load_valid_data(test_file,classes,relations)\n",
    "test_data = torch.Tensor(test_data).long().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 0.003861003861003861 0.04141804141804142 8300.706154206155 5131.0 22358.4 0.8191109940047732\n"
     ]
    }
   ],
   "source": [
    "\n",
    "top1,top10, top100, mean_rank, median_rank, per_rank, auc,rank_values = compute_rank(model, test_data, 90)\n",
    "print(top1,top10, top100, mean_rank, median_rank, per_rank, auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 0.02800131384463787 0.08720643783872557 18764.159919527017 8447.5 62722.70000000002 0.8235613038053\n"
     ]
    }
   ],
   "source": [
    "\n",
    "top1,top10, top100, mean_rank, median_rank, per_rank, auc,rank_values = compute_rank(model, test_data, 90)\n",
    "print(top1,top10, top100, mean_rank, median_rank, per_rank, auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.22770569880111677\n"
     ]
    }
   ],
   "source": [
    "print(compute_accuracy(model, test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "box",
   "language": "python",
   "name": "box"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
