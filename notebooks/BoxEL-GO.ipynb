{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "import wandb\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import math\n",
    "import argparse\n",
    "import os\n",
    "import json\n",
    "from utils import *\n",
    "from dataset import *\n",
    "from softbox import SoftBox\n",
    "from gumbel_box import GumbelBox\n",
    "\n",
    "torch.cuda.set_device(1)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "# data parameters\n",
    "parser.add_argument('--train_data_path', type=str, default='./data/full_wordnet/full_wordnet_noneg.tsv', help='path to train data')\n",
    "parser.add_argument('--test_data_path', type=str, default='./data/full_wordnet/full_wordnet.tsv', help='path to test data')\n",
    "parser.add_argument('--vocab_path', type=str, default='./data/full_wordnet/full_wordnet_vocab.tsv', help='path to vocab')\n",
    "# training parameters\n",
    "parser.add_argument('--no_cuda', action='store_true', default=False, help='disables CUDA training (eg. no nvidia GPU)')\n",
    "parser.add_argument('--random_negative_sampling_ratio', type=int, default=1, help='sample this many random negatives for each positive.')\n",
    "parser.add_argument('--log_batch_size', type=int, default=13, help='batch size for training will be 2**LOG_BATCH_SIZE')\n",
    "parser.add_argument('--learning_rate', type=float, default=5e-3, help='learning rate')\n",
    "parser.add_argument('--epochs', type=int, default=10000, help='number of epochs to train')\n",
    "parser.add_argument('--prediction_thres', type=float, default=0.8, help='the probability threshold for prediction')\n",
    "# model parameters\n",
    "parser.add_argument('--model', type=str, default='softbox', help='model type: choose from softbox, gumbel')\n",
    "parser.add_argument('--box_embedding_dim', type=int, default=40, help='box embedding dimension')\n",
    "parser.add_argument('--softplus_temp', type=float, default=1.0, help='beta of softplus function')\n",
    "# gumbel box parameters\n",
    "parser.add_argument('--gumbel_beta', type=float, default=1.0, help='beta value for gumbel distribution')\n",
    "parser.add_argument('--scale', type=float, default=1.0, help='scale value for gumbel distribution')\n",
    "# a parameter can be set to a model checkpoint path or left as None\n",
    "# If set, the checkpoint will be resumed and tested; the user needs to specify test_data_path and vocab_path but not others\n",
    "# Other parameters will be restored from the model checkpoint\n",
    "parser.add_argument('--resume_and_test', type=str, default=None, help='path to a model checkpoint to be resumed and tested')\n",
    "\n",
    "args = parser.parse_args(args=['--no_cuda'] )\n",
    "args.save_to = \"./checkpoints/\" + args.model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "eps = 1e-8\n",
    "\n",
    "def l2_side_regularizer(box, log_scale: bool = True):\n",
    "    \"\"\"Applies l2 regularization on all sides of all boxes and returns the sum.\n",
    "    \"\"\"\n",
    "    min_x = box.min_embed \n",
    "    delta_x = box.delta_embed  \n",
    "\n",
    "    if not log_scale:\n",
    "        return torch.mean(delta_x ** 2)\n",
    "    else:\n",
    "        return  torch.mean(F.relu(min_x + delta_x - 1 + eps )) + F.relu(torch.norm(min_x, p=2)-1)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import wandb\n",
    "import torch.nn as nn\n",
    "from basic_box import Box\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import uniform\n",
    "\n",
    "euler_gamma = 0.57721566490153286060\n",
    "\n",
    "gamma = 1\n",
    "\n",
    "class BoxEL(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, relation_size,embed_dim, min_init_value, delta_init_value, relation_init_value, scaling_init_value, args):\n",
    "        super(BoxEL, self).__init__()\n",
    "        min_embedding = self.init_concept_embedding(vocab_size, embed_dim, min_init_value)\n",
    "        delta_embedding = self.init_concept_embedding(vocab_size, embed_dim, delta_init_value)\n",
    "        relation_embedding = self.init_concept_embedding(relation_size, embed_dim, relation_init_value)\n",
    "        scaling_embedding = self.init_concept_embedding(relation_size, embed_dim, scaling_init_value)\n",
    "        \n",
    "        self.temperature = args.softplus_temp\n",
    "        self.min_embedding = nn.Parameter(min_embedding)\n",
    "        self.delta_embedding = nn.Parameter(delta_embedding)\n",
    "        self.relation_embedding = nn.Parameter(relation_embedding)\n",
    "        self.scaling_embedding = nn.Parameter(scaling_embedding)\n",
    "        \n",
    "        self.gumbel_beta = args.gumbel_beta\n",
    "        self.scale = args.scale\n",
    "\n",
    "    def forward(self, data):\n",
    "        nf1_min = self.min_embedding[data[0][:,[0,2]]]\n",
    "        nf1_delta = self.delta_embedding[data[0][:,[0,2]]]\n",
    "        nf1_max = nf1_min+torch.exp(nf1_delta)\n",
    "        \n",
    "        boxes1 = Box(nf1_min[:, 0, :], nf1_max[:, 0, :])\n",
    "        boxes2 = Box(nf1_min[:, 1, :], nf1_max[:, 1, :])\n",
    "        \n",
    "        nf1_loss, nf1_reg_loss = self.nf1_loss(boxes1, boxes2)\n",
    "        \n",
    "        nf2_min = self.min_embedding[data[1]]\n",
    "        nf2_delta = self.delta_embedding[data[1]]\n",
    "        nf2_max = nf2_min+torch.exp(nf2_delta)\n",
    "        \n",
    "        boxes1 = Box(nf2_min[:, 0, :], nf2_max[:, 0, :])\n",
    "        boxes2 = Box(nf2_min[:, 1, :], nf2_max[:, 1, :])\n",
    "        boxes3 = Box(nf2_min[:, 2, :], nf2_max[:, 2, :])\n",
    "        \n",
    "        nf2_loss,nf2_reg_loss = self.nf2_loss(boxes1, boxes2, boxes3)\n",
    "        \n",
    "        nf3_min = self.min_embedding[data[2][:,[0,2]]]\n",
    "        nf3_delta = self.delta_embedding[data[2][:,[0,2]]]\n",
    "        nf3_max = nf3_min+torch.exp(nf3_delta)\n",
    "        relation = self.relation_embedding[data[2][:,1]]\n",
    "        scaling = self.scaling_embedding[data[2][:,1]]\n",
    "        \n",
    "        boxes1 = Box(nf3_min[:, 0, :], nf3_max[:, 0, :])\n",
    "        boxes2 = Box(nf3_min[:, 1, :], nf3_max[:, 1, :])\n",
    "        \n",
    "        nf3_loss,nf3_reg_loss = self.nf3_loss(boxes1, relation, scaling, boxes2)\n",
    "        \n",
    "        nf4_min = self.min_embedding[data[3][:,1:]]\n",
    "        nf4_delta = self.delta_embedding[data[3][:,1:]]\n",
    "        nf4_max = nf4_min+torch.exp(nf4_delta)\n",
    "        relation = self.relation_embedding[data[3][:,0]]\n",
    "        scaling = self.scaling_embedding[data[3][:,0]]\n",
    "        \n",
    "        boxes1 = Box(nf4_min[:, 0, :], nf4_max[:, 0, :])\n",
    "        boxes2 = Box(nf4_min[:, 1, :], nf4_max[:, 1, :])\n",
    "        \n",
    "        nf4_loss,nf4_reg_loss = self.nf4_loss(relation, scaling, boxes1, boxes2)\n",
    "        \n",
    "        disjoint_min = self.min_embedding[data[4]]\n",
    "        disjoint_delta = self.delta_embedding[data[4]]\n",
    "        disjoint_max = disjoint_min+torch.exp(disjoint_delta)\n",
    "        boxes1 = Box(disjoint_min[:, 0, :], disjoint_max[:, 0, :])\n",
    "        boxes2 = Box(disjoint_min[:, 1, :], disjoint_max[:, 1, :])\n",
    "        disjoint_loss,disjoint_reg_loss = self.disjoint_loss(boxes1, boxes2)\n",
    "        \n",
    "        nf3_neg_min = self.min_embedding[data[6][:,[0,2]]]\n",
    "        nf3_neg_delta = self.delta_embedding[data[6][:,[0,2]]]\n",
    "        nf3_neg_max = nf3_neg_min+torch.exp(nf3_neg_delta)\n",
    "        \n",
    "        relation = self.relation_embedding[data[6][:,1]]\n",
    "        scaling = self.scaling_embedding[data[6][:,1]]\n",
    "        \n",
    "        boxes1 = Box(nf3_neg_min[:, 0, :], nf3_neg_max[:, 0, :])\n",
    "        boxes2 = Box(nf3_neg_min[:, 1, :], nf3_neg_max[:, 1, :])\n",
    "        \n",
    "        nf3_neg_loss,nf3_neg_reg_loss = self.nf3_neg_loss(boxes1, relation, scaling, boxes2)\n",
    "        \n",
    "        all_min = self.min_embedding\n",
    "        all_delta = self.delta_embedding\n",
    "        all_max = all_min+torch.exp(all_delta)\n",
    "        boxes = Box(all_min, all_max)\n",
    "        reg_loss = l2_side_regularizer(boxes, log_scale=True)\n",
    "        \n",
    "        return nf1_loss.sum(), nf2_loss.sum(), nf3_loss.sum(), nf4_loss.sum(), disjoint_loss.sum(), nf3_neg_loss.sum(), nf1_reg_loss, nf2_reg_loss , nf3_reg_loss , nf4_reg_loss , disjoint_reg_loss , nf3_neg_reg_loss\n",
    "\n",
    "    def get_cond_probs(self, data):\n",
    "        nf3_min = self.min_embedding[data[:,[0,2]]]\n",
    "        nf3_delta = self.delta_embedding[data[:,[0,2]]]\n",
    "        nf3_max = nf3_min+torch.exp(nf3_delta)\n",
    "        \n",
    "        relation = self.relation_embedding[data[:,1]]\n",
    "        \n",
    "        boxes1 = Box(nf3_min[:, 0, :], nf3_max[:, 0, :])\n",
    "        boxes2 = Box(nf3_min[:, 1, :], nf3_max[:, 1, :])\n",
    "        \n",
    "        log_intersection = torch.log(torch.clamp(self.volumes(self.intersection(boxes1, boxes2)), 1e-10, 1e4))\n",
    "        log_box2 = torch.log(torch.clamp(self.volumes(boxes2), 1e-10, 1e4))\n",
    "        return torch.exp(log_intersection-log_box2)\n",
    "        \n",
    "\n",
    "    def volumes(self, boxes):\n",
    "        return F.softplus(boxes.delta_embed, beta=self.temperature).prod(1)\n",
    "\n",
    "    def intersection(self, boxes1, boxes2):\n",
    "        intersections_min = torch.max(boxes1.min_embed, boxes2.min_embed)\n",
    "        intersections_max = torch.min(boxes1.max_embed, boxes2.max_embed)\n",
    "        intersection_box = Box(intersections_min, intersections_max)\n",
    "        return intersection_box\n",
    "    \n",
    "    def inclusion_loss(self, boxes1, boxes2):\n",
    "        log_intersection = torch.log(torch.clamp(self.volumes(self.intersection(boxes1, boxes2)), 1e-10, 1e4))\n",
    "        log_box1 = torch.log(torch.clamp(self.volumes(boxes1), 1e-10, 1e4))\n",
    "        \n",
    "        return 1-torch.exp(log_intersection-log_box1)\n",
    "    \n",
    "    def nf1_loss(self, boxes1, boxes2):\n",
    "        return self.inclusion_loss(boxes1, boxes2), l2_side_regularizer(boxes1, log_scale=True) + l2_side_regularizer(boxes2, log_scale=True)\n",
    "        \n",
    "    def nf2_loss(self, boxes1, boxes2, boxes3):\n",
    "        inter_box = self.intersection(boxes1, boxes2)\n",
    "        return self.inclusion_loss(inter_box, boxes3), l2_side_regularizer(inter_box, log_scale=True) + l2_side_regularizer(boxes1, log_scale=True) + l2_side_regularizer(boxes2, log_scale=True) + l2_side_regularizer(boxes3, log_scale=True)\n",
    "    \n",
    "    def nf3_loss(self, boxes1, relation, scaling, boxes2):\n",
    "        trans_min = boxes1.min_embed*(scaling + eps) + relation\n",
    "        trans_max = boxes1.max_embed*(scaling + eps) + relation\n",
    "        trans_boxes = Box(trans_min, trans_max)\n",
    "        return self.inclusion_loss(trans_boxes, boxes2), l2_side_regularizer(trans_boxes, log_scale=True) + l2_side_regularizer(boxes1, log_scale=True) + l2_side_regularizer(boxes2, log_scale=True) \n",
    "    \n",
    "    def nf4_loss(self, relation, scaling, boxes1, boxes2):\n",
    "        trans_min = (boxes1.min_embed - relation)/(scaling + eps)\n",
    "        trans_max = (boxes1.max_embed - relation)/(scaling + eps)\n",
    "        trans_boxes = Box(trans_min, trans_max)\n",
    "#         log_trans_boxes = torch.log(torch.clamp(self.volumes(trans_boxes), 1e-10, 1e4))\n",
    "        return self.inclusion_loss(trans_boxes, boxes2), l2_side_regularizer(trans_boxes, log_scale=True) + l2_side_regularizer(boxes1, log_scale=True) + l2_side_regularizer(boxes2, log_scale=True) \n",
    "        \n",
    "    def disjoint_loss(self, boxes1, boxes2):\n",
    "        log_intersection = torch.log(torch.clamp(self.volumes(self.intersection(boxes1, boxes2)), 1e-10, 1e4))\n",
    "        log_boxes1 = torch.log(torch.clamp(self.volumes(boxes1), 1e-10, 1e4))\n",
    "        log_boxes2 = torch.log(torch.clamp(self.volumes(boxes2), 1e-10, 1e4))\n",
    "        union = log_boxes1 + log_boxes2\n",
    "        return torch.exp(log_intersection-union), l2_side_regularizer(boxes1, log_scale=True) + l2_side_regularizer(boxes2, log_scale=True)\n",
    "        \n",
    "    def nf3_neg_loss(self, boxes1, relation, scaling, boxes2):\n",
    "        trans_min = boxes1.min_embed*(scaling + eps) + relation\n",
    "        trans_max = boxes1.max_embed*(scaling + eps) + relation\n",
    "        trans_boxes = Box(trans_min, trans_max)\n",
    "#         trans_min = boxes1.min_embed + relation\n",
    "#         trans_max = trans_min + torch.clamp((boxes1.max_embed - boxes1.min_embed)*(scaling + eps), 1e-10, 1e4)\n",
    "#         trans_boxes = Box(trans_min, trans_max)\n",
    "        return 1-self.inclusion_loss(trans_boxes, boxes2),l2_side_regularizer(trans_boxes, log_scale=True) + l2_side_regularizer(boxes1, log_scale=True) + l2_side_regularizer(boxes2, log_scale=True) \n",
    "        \n",
    "    def init_concept_embedding(self, vocab_size, embed_dim, init_value):\n",
    "        distribution = uniform.Uniform(init_value[0], init_value[1])\n",
    "        box_embed = distribution.sample((vocab_size, embed_dim))\n",
    "        return box_embed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"  \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "import click as ck\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "from scipy.stats import rankdata\n",
    "\n",
    "def load_valid_data(valid_data_file, classes, relations):\n",
    "    data = []\n",
    "    rel = f'SubClassOf'\n",
    "    with open(valid_data_file, 'r') as f:\n",
    "        for line in f:\n",
    "            it = line.strip().split()\n",
    "            id1 = it[0]\n",
    "            id2 = it[1]\n",
    "            if id1 not in classes or id2 not in classes or rel not in relations:\n",
    "                continue\n",
    "            data.append((classes[id1], relations[rel], classes[id2]))\n",
    "    return data\n",
    "\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "\n",
    "def load_cls(train_data_file):\n",
    "    train_subs=list()\n",
    "    counter=0\n",
    "    with open(train_data_file,'r') as f:\n",
    "        for line in f:\n",
    "            counter+=1\n",
    "            it = line.strip().split()\n",
    "            cls1 = it[0]\n",
    "            cls2 = it[1]\n",
    "            train_subs.append(cls1)\n",
    "            train_subs.append(cls2)\n",
    "    train_cls = list(set(train_subs))\n",
    "    return train_cls,counter\n",
    "\n",
    "\n",
    "#Original Loss\n",
    "def load_data(filename):\n",
    "    classes = {}\n",
    "    relations = {}\n",
    "    data = {'nf1': [], 'nf2': [], 'nf3': [], 'nf4': [], 'disjoint': []}\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            # Ignore SubObjectPropertyOf\n",
    "            if line.startswith('SubObjectPropertyOf'):\n",
    "                continue\n",
    "            # Ignore SubClassOf()\n",
    "            line = line.strip()[11:-1]\n",
    "            if not line:\n",
    "                continue\n",
    "            if line.startswith('ObjectIntersectionOf('):\n",
    "                # C and D SubClassOf E\n",
    "                it = line.split(' ')\n",
    "                c = it[0][21:]\n",
    "                d = it[1][:-1]\n",
    "                e = it[2]\n",
    "                if c not in classes:\n",
    "                    classes[c] = len(classes)\n",
    "                if d not in classes:\n",
    "                    classes[d] = len(classes)\n",
    "                if e not in classes:\n",
    "                    classes[e] = len(classes)\n",
    "                form = 'nf2'\n",
    "                if e == 'owl:Nothing':\n",
    "                    form = 'disjoint'\n",
    "                data[form].append((classes[c], classes[d], classes[e]))\n",
    "                \n",
    "            elif line.startswith('ObjectSomeValuesFrom('):\n",
    "                # R some C SubClassOf D\n",
    "                it = line.split(' ')\n",
    "                r = it[0][21:]\n",
    "                c = it[1][:-1]\n",
    "                d = it[2]\n",
    "                if c not in classes:\n",
    "                    classes[c] = len(classes)\n",
    "                if d not in classes:\n",
    "                    classes[d] = len(classes)\n",
    "                if r not in relations:\n",
    "                    relations[r] = len(relations)\n",
    "                data['nf4'].append((relations[r], classes[c], classes[d]))\n",
    "            elif line.find('ObjectSomeValuesFrom') != -1:\n",
    "                # C SubClassOf R some D\n",
    "                it = line.split(' ')\n",
    "                c = it[0]\n",
    "                r = it[1][21:]\n",
    "                d = it[2][:-1]\n",
    "                if c not in classes:\n",
    "                    classes[c] = len(classes)\n",
    "                if d not in classes:\n",
    "                    classes[d] = len(classes)\n",
    "                if r not in relations:\n",
    "                    relations[r] = len(relations)\n",
    "                data['nf3'].append((classes[c], relations[r], classes[d]))\n",
    "            else:\n",
    "                # C SubClassOf D\n",
    "                it = line.split(' ')\n",
    "                c = it[0]\n",
    "                d = it[1]\n",
    "                r = 'SubClassOf'\n",
    "                if r not in relations:\n",
    "                    relations[r] = len(relations)\n",
    "                if c not in classes:\n",
    "                    classes[c] = len(classes)\n",
    "                if d not in classes:\n",
    "                    classes[d] = len(classes)\n",
    "                data['nf1'].append((classes[c],relations[r],classes[d]))\n",
    "                \n",
    "    # Check if TOP in classes and insert if it is not there\n",
    "    if 'owl:Thing' not in classes:\n",
    "        classes['owl:Thing'] = len(classes)\n",
    "#changing by adding sub classes of train_data ids to prot_ids\n",
    "    prot_ids = []\n",
    "    class_keys = list(classes.keys())\n",
    "    for val in all_subcls:\n",
    "        if val not in class_keys:\n",
    "            cid = len(classes)\n",
    "            classes[val] = cid\n",
    "            prot_ids.append(cid)\n",
    "        else:\n",
    "            prot_ids.append(classes[val])\n",
    "\n",
    "    prot_ids = np.array(prot_ids)\n",
    "    \n",
    "    \n",
    "    # Add corrupted triples nf3\n",
    "    n_classes = len(classes)\n",
    "    data['nf3_neg'] = []\n",
    "    for c, r, d in data['nf3']:\n",
    "        x = np.random.choice(prot_ids)\n",
    "        while x == c:\n",
    "            x = np.random.choice(prot_ids)\n",
    "            \n",
    "        y = np.random.choice(prot_ids)\n",
    "        while y == d:\n",
    "             y = np.random.choice(prot_ids)\n",
    "        data['nf3_neg'].append((c, r,x))\n",
    "        data['nf3_neg'].append((y, r, d))\n",
    "        \n",
    "    \n",
    "    data['nf1'] = np.array(data['nf1'])\n",
    "    data['nf2'] = np.array(data['nf2'])\n",
    "    data['nf3'] = np.array(data['nf3'])\n",
    "    data['nf4'] = np.array(data['nf4'])\n",
    "    data['disjoint'] = np.array(data['disjoint'])\n",
    "    data['top'] = np.array([classes['owl:Thing'],])\n",
    "    data['nf3_neg'] = np.array(data['nf3_neg'])\n",
    "                            \n",
    "    for key, val in data.items():\n",
    "        index = np.arange(len(data[key]))\n",
    "        np.random.seed(seed=100)\n",
    "        np.random.shuffle(index)\n",
    "        data[key] = val[index]\n",
    "    \n",
    "    return data, classes, relations\n",
    "\n",
    "\n",
    "class Generator(object):\n",
    "    def __init__(self, data, batch_size=128, steps=100):\n",
    "        self.data = data\n",
    "        self.batch_size = batch_size\n",
    "        self.steps = steps\n",
    "        self.start = 0\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        return self.next()\n",
    "\n",
    "    def reset(self):\n",
    "        self.start = 0\n",
    "\n",
    "    def next(self):\n",
    "        if self.start < self.steps:\n",
    "            nf1_index = np.random.choice(\n",
    "                self.data['nf1'].shape[0], self.batch_size)\n",
    "            nf2_index = np.random.choice(\n",
    "                self.data['nf2'].shape[0], self.batch_size)\n",
    "            nf3_index = np.random.choice(\n",
    "                self.data['nf3'].shape[0], self.batch_size)\n",
    "            nf4_index = np.random.choice(\n",
    "                self.data['nf4'].shape[0], self.batch_size)\n",
    "            dis_index = np.random.choice(\n",
    "                self.data['disjoint'].shape[0], self.batch_size)\n",
    "            top_index = np.random.choice(\n",
    "                self.data['top'].shape[0], self.batch_size)\n",
    "            nf3_neg_index = np.random.choice(\n",
    "                self.data['nf3_neg'].shape[0], self.batch_size)\n",
    "            nf1 = self.data['nf1'][nf1_index]\n",
    "            nf2 = self.data['nf2'][nf2_index]\n",
    "            nf3 = self.data['nf3'][nf3_index]\n",
    "            nf4 = self.data['nf4'][nf4_index]\n",
    "            dis = self.data['disjoint'][dis_index]\n",
    "            top = self.data['top'][top_index]\n",
    "            nf3_neg = self.data['nf3_neg'][nf3_neg_index]\n",
    "            labels = np.zeros((self.batch_size, 1), dtype=np.float32)\n",
    "            self.start += 1\n",
    "            return ([nf1, nf2, nf3, nf4, dis, top, nf3_neg], labels)\n",
    "        else:\n",
    "            self.reset()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data samples: 59829\n",
      "Training data classes: 38846\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset = 'GO'\n",
    "\n",
    "total_sub_cls=[]\n",
    "train_file = f\"data/{dataset}/{dataset}_train.txt\"\n",
    "va_file = f\"data/{dataset}/{dataset}_valid.txt\"\n",
    "test_file = f\"data/{dataset}/{dataset}_test.txt\"\n",
    "train_sub_cls,train_samples = load_cls(train_file)\n",
    "valid_sub_cls,valid_samples = load_cls(va_file)\n",
    "test_sub_cls,test_samples = load_cls(test_file)\n",
    "total_sub_cls = train_sub_cls + valid_sub_cls + test_sub_cls\n",
    "all_subcls = list(set(total_sub_cls))\n",
    "\n",
    "print(\"Training data samples:\",train_samples)\n",
    "print(\"Training data classes:\",len(train_sub_cls))\n",
    "\n",
    "gdata_file=f\"data/{dataset}/{dataset}_latest_norm_mod.owl\"\n",
    "train_data, classes, relations = load_data(gdata_file)\n",
    "valid_data_file=f\"data/{dataset}/{dataset}_valid.txt\"\n",
    "valid_data = load_valid_data(valid_data_file, classes, relations)\n",
    "valid_data = torch.Tensor(valid_data).long().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nf1': array([[19424,     0, 19425],\n",
       "        [26074,     0, 16032],\n",
       "        [16641,     0, 13075],\n",
       "        ...,\n",
       "        [29999,     0,  1805],\n",
       "        [ 3424,     0,  6978],\n",
       "        [37048,     0, 14876]]), 'nf2': array([[    1,  5452,  5453],\n",
       "        [    1, 24285, 16198],\n",
       "        [13254, 35387,  7855],\n",
       "        ...,\n",
       "        [    1, 24344,  7121],\n",
       "        [33770, 39590,  1341],\n",
       "        [    1, 10572,  4612]]), 'nf3': array([[15783,     1, 21340],\n",
       "        [29112,     1, 33558],\n",
       "        [37930,     2, 37931],\n",
       "        ...,\n",
       "        [25780,     1,  7996],\n",
       "        [ 1903,     1,  4752],\n",
       "        [20132,     1,  4912]]), 'nf4': array([[    3,  3322,  5760],\n",
       "        [    4, 24922, 20717],\n",
       "        [    1,  3667, 27390],\n",
       "        ...,\n",
       "        [    2, 16875, 21040],\n",
       "        [    3, 13193, 18021],\n",
       "        [    1, 29067, 37128]]), 'disjoint': array([[36465, 16873, 17238],\n",
       "        [17237, 16715, 17238],\n",
       "        [43487, 43454, 17238],\n",
       "        [ 6612, 17237, 17238],\n",
       "        [ 6221, 18906, 17238],\n",
       "        [22909, 17237, 17238],\n",
       "        [42604, 42603, 17238],\n",
       "        [17237,  2161, 17238],\n",
       "        [11682, 28033, 17238],\n",
       "        [17237, 35096, 17238],\n",
       "        [24803, 24804, 17238],\n",
       "        [23691, 17237, 17238],\n",
       "        [17237,  4149, 17238],\n",
       "        [23857,  1313, 17238],\n",
       "        [ 8080, 17237, 17238],\n",
       "        [ 7595, 25038, 17238],\n",
       "        [25038, 17237, 17238],\n",
       "        [  932, 17237, 17238],\n",
       "        [15463, 17237, 17238],\n",
       "        [17237,  2784, 17238],\n",
       "        [14158, 14159, 17238],\n",
       "        [27683, 17237, 17238],\n",
       "        [ 2697, 17237, 17238],\n",
       "        [37957, 17237, 17238],\n",
       "        [16509, 17237, 17238],\n",
       "        [  292, 17237, 17238],\n",
       "        [25983, 15939, 17238],\n",
       "        [17237, 32272, 17238],\n",
       "        [17237, 14148, 17238],\n",
       "        [36465, 10417, 17238]]), 'nf3_neg': array([[40584,     2, 23781],\n",
       "        [34018,     1, 34646],\n",
       "        [18690,     4,  3206],\n",
       "        ...,\n",
       "        [10463,     4, 11306],\n",
       "        [39872,     2, 23918],\n",
       "        [14731,     4, 35851]]), 'top': array([45109])}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no. classes: 45895\n",
      "no. relations: 9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([45895, 1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 256 \n",
    "\n",
    "proteins = {} # substitute for classes with subclass case\n",
    "for val in all_subcls:\n",
    "    proteins[val] = classes[val]\n",
    "nb_classes = len(classes)\n",
    "nb_relations = len(relations)\n",
    "\n",
    "print(\"no. classes:\",nb_classes)\n",
    "print(\"no. relations:\",nb_relations)\n",
    "nb_train_data = 0\n",
    "\n",
    "for key, val in train_data.items():\n",
    "    nb_train_data = max(len(val), nb_train_data)\n",
    "train_steps = int(math.ceil(nb_train_data / (1.0 * batch_size)))\n",
    "train_generator = Generator(train_data, batch_size, steps=train_steps)\n",
    "\n",
    "cls_dict = {v: k for k, v in classes.items()}\n",
    "rel_dict = {v: k for k, v in relations.items()}\n",
    "\n",
    "cls_list = []\n",
    "rel_list = []\n",
    "for i in range(nb_classes):\n",
    "    cls_list.append(cls_dict[i])\n",
    "for i in range(nb_relations):\n",
    "    rel_list.append(rel_dict[i])\n",
    "\n",
    "classes_index = list(classes.values())\n",
    "classes_index = torch.Tensor(classes_index).to(device).reshape(-1,1).long()\n",
    "classes_index.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Currently logged in as: boxiong (use `wandb login --relogin` to force relogin)\n",
      "wandb: wandb version 0.12.4 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.12.0<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">volcanic-pegasus-2</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/boxiong/GO\" target=\"_blank\">https://wandb.ai/boxiong/GO</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/boxiong/GO/runs/hynjy3ac\" target=\"_blank\">https://wandb.ai/boxiong/GO/runs/hynjy3ac</a><br/>\n",
       "                Run data is saved locally in <code>/workspace/geometric/box_el/wandb/run-20211013_132812-hynjy3ac</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h1>Run(hynjy3ac)</h1><iframe src=\"https://wandb.ai/boxiong/GO/runs/hynjy3ac\" style=\"border:none;width:100%;height:400px\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fa7201b0550>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=\"GO\", reinit=True, config=args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model = BoxEL(nb_classes, nb_relations, 50, [1e-4, 0.2], [-0.1, 0], [-0.1,0.1], [0.9,1.1], args).to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "wandb.watch(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1 Train loss: 259.739394 Valid Mean Rank: 21266.215\n",
      "\n",
      "Epoch:101 Train loss: 7.956819 Valid Mean Rank: 7646.175\n",
      "\n",
      "Epoch:201 Train loss: 6.430723 Valid Mean Rank: 5270.085\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-ab18c898ccf1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0mnf1_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnf2_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnf3_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnf4_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisjoint_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnf3_neg_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnf1_reg_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnf2_reg_loss\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mnf3_reg_loss\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mnf4_reg_loss\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mdisjoint_reg_loss\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mnf3_neg_reg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mnf1_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnf1_reg_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnf2_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnf2_reg_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdisjoint_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdisjoint_reg_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnf3_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnf3_reg_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnf4_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnf4_reg_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnf3_neg_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnf3_neg_reg_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/anaconda3/envs/box/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-479ed3599c32>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mboxes2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBox\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnf3_min\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnf3_max\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0mnf3_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnf3_reg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnf3_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboxes1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrelation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaling\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboxes2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mnf4_min\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_embedding\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-479ed3599c32>\u001b[0m in \u001b[0;36mnf3_loss\u001b[0;34m(self, boxes1, relation, scaling, boxes2)\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0mtrans_max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mboxes1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_embed\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscaling\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrelation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mtrans_boxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBox\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrans_min\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrans_max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minclusion_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrans_boxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboxes2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml2_side_regularizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrans_boxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_scale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ml2_side_regularizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboxes1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_scale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ml2_side_regularizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboxes2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_scale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnf4_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrelation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaling\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboxes1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboxes2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-8d7d9f3585ce>\u001b[0m in \u001b[0;36ml2_side_regularizer\u001b[0;34m(box, log_scale)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelta_x\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0;32mreturn\u001b[0m  \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_x\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdelta_x\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0meps\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/anaconda3/envs/box/lib/python3.6/site-packages/torch/functional.py\u001b[0m in \u001b[0;36mnorm\u001b[0;34m(input, p, dim, keepdim, out, dtype)\u001b[0m\n\u001b[1;32m   1374\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1375\u001b[0m             \u001b[0m_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# noqa: C416 TODO: rewrite as list(range(m))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1376\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeepdim\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1378\u001b[0m     \u001b[0;31m# TODO: when https://github.com/pytorch/pytorch/issues/33782 is fixed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "torch.manual_seed(2222)\n",
    "import random\n",
    "random.seed(2222)\n",
    "\n",
    "model.train()\n",
    "steps_per_epoch = train_steps\n",
    "train_generator.reset()\n",
    "\n",
    "epochs = 1000\n",
    "for epoch in range(epochs):\n",
    "    train_loss = 0.0\n",
    "    nf1_total_loss = 0.0\n",
    "    nf2_total_loss = 0.0\n",
    "    nf3_total_loss = 0.0\n",
    "    nf4_total_loss = 0.0\n",
    "    disjoint_total_loss = 0.0\n",
    "    nf3_neg_total_loss = 0.0\n",
    "    \n",
    "    for step, batch in enumerate(train_generator):\n",
    "        if step < steps_per_epoch:\n",
    "            nf1_loss, nf2_loss, nf3_loss, nf4_loss, disjoint_loss, nf3_neg_loss, nf1_reg_loss, nf2_reg_loss , nf3_reg_loss , nf4_reg_loss , disjoint_reg_loss , nf3_neg_reg_loss = model(batch[0])\n",
    "            loss =  nf1_loss + nf1_reg_loss + nf2_loss + nf2_reg_loss + 10*disjoint_loss + disjoint_reg_loss + nf3_loss + nf3_reg_loss + nf4_loss + nf4_reg_loss + nf3_neg_loss + nf3_neg_reg_loss\n",
    "            assert torch.isnan(loss).sum() == 0\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            assert torch.isnan(model.min_embedding).sum() == 0\n",
    "            optimizer.step()\n",
    "            assert torch.isnan(model.min_embedding).sum() == 0\n",
    "            assert torch.isnan(model.min_embedding.grad).sum() == 0\n",
    "            train_loss += loss\n",
    "            nf1_total_loss += nf1_loss\n",
    "            nf2_total_loss += nf2_loss\n",
    "            nf3_total_loss += nf3_loss\n",
    "            nf4_total_loss += nf4_loss\n",
    "            disjoint_total_loss += disjoint_loss\n",
    "            nf3_neg_total_loss += nf3_neg_loss\n",
    "        else:\n",
    "            train_generator.reset()\n",
    "            break\n",
    "            \n",
    "    mean_rank = compute_mean_rank(model, valid_data[0:100])\n",
    "    valid_accuracy = compute_accuracy(model, valid_data)\n",
    "    \n",
    "    wandb.log({'train loss': train_loss.item()/(step+1),'nf1_loss':nf1_total_loss.item()/(step+1), 'nf2_loss':nf2_total_loss.item()/(step+1), 'nf3_loss':nf3_total_loss.item()/(step+1), 'nf4_loss':nf4_total_loss.item()/(step+1), 'nf3_neg_loss':nf3_neg_total_loss.item()/(step+1), 'mean_rank':mean_rank,'valid_accuracy':valid_accuracy })\n",
    "\n",
    "    PATH = './models/box_el_go_50_seed2222.pt'\n",
    "    if epoch % 100 == 0:\n",
    "        torch.save(model, PATH)\n",
    "        # compute_mean_rank(model, valid_data[0:1000])\n",
    "        print('Epoch:%d' %(epoch + 1), \"Train loss: %f\" %(train_loss.item()/(step+1)), f'Valid Mean Rank: {mean_rank}\\n')\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from scipy.stats import rankdata\n",
    "import statistics\n",
    "from collections import Counter\n",
    "\n",
    "def compute_cond_probs(model, boxes1, boxes2):\n",
    "    log_intersection = torch.log(torch.clamp(model.volumes(model.intersection(boxes1, boxes2)), 1e-10, 1e4))\n",
    "    log_box2 = torch.log(torch.clamp(model.volumes(boxes2), 1e-10, 1e4))\n",
    "    return torch.exp(log_intersection-log_box2)\n",
    "\n",
    "def compute_mean_rank(model, valid_data):\n",
    "    mean_rank = 0.0\n",
    "    n = len(valid_data)\n",
    "    for i, (c, r, d) in enumerate(valid_data):\n",
    "        c_data = torch.cat((c.repeat(classes_index.shape[0], 1), torch.Tensor([0]).repeat(classes_index.shape[0], 1).to(device).long(), classes_index), 1) \n",
    "        nf1_min = model.min_embedding[c_data[:,[0,2]]]\n",
    "        nf1_delta = model.delta_embedding[c_data[:,[0,2]]]\n",
    "        nf1_max = nf1_min+torch.exp(nf1_delta)\n",
    "        boxes1 = Box(nf1_min[:, 0, :], nf1_max[:, 0, :])\n",
    "        boxes2 = Box(nf1_min[:, 1, :], nf1_max[:, 1, :])\n",
    "        c_probs = 1- compute_cond_probs(model, boxes1, boxes2).cpu().detach().numpy()\n",
    "        index = rankdata(c_probs, method='average')\n",
    "        rank = index[d]\n",
    "        mean_rank += rank \n",
    "    mean_rank /= n\n",
    "    return mean_rank\n",
    "\n",
    "def compute_rank(model, valid_data, ratio):\n",
    "    rank_values = []\n",
    "    top1 = 0\n",
    "    top10 = 0\n",
    "    top100 = 0\n",
    "    n = len(valid_data)\n",
    "    rank_percentile = []\n",
    "    for i, (c, r, d) in enumerate(valid_data):\n",
    "        c_data = torch.cat((c.repeat(classes_index.shape[0], 1), torch.Tensor([0]).repeat(classes_index.shape[0], 1).to(device).long(), classes_index), 1) \n",
    "        nf1_min = model.min_embedding[c_data[:,[0,2]]]\n",
    "        nf1_delta = model.delta_embedding[c_data[:,[0,2]]]\n",
    "        nf1_max = nf1_min+torch.exp(nf1_delta)\n",
    "        boxes1 = Box(nf1_min[:, 0, :], nf1_max[:, 0, :])\n",
    "        boxes2 = Box(nf1_min[:, 1, :], nf1_max[:, 1, :])\n",
    "        c_probs = 1- compute_cond_probs(model, boxes1, boxes2).cpu().detach().numpy()\n",
    "        index = rankdata(c_probs, method='average')\n",
    "        rank = index[d]\n",
    "        rank_values.append(rank)\n",
    "        rank_percentile.append(rank)\n",
    "        if rank == 1:\n",
    "            top1 += 1\n",
    "        if rank <= 10:\n",
    "            top10 += 1\n",
    "        if rank <= 100:\n",
    "            top100 += 1\n",
    "    \n",
    "    top1 /= (i+1)\n",
    "    top10 /= (i+1)\n",
    "    top100 /= (i+1)\n",
    "    \n",
    "    mean_rank = np.mean(rank_values)\n",
    "    median_rank = statistics.median(rank_values)\n",
    "    rank_percentile.sort()\n",
    "    per_rank = np.percentile(rank_percentile,ratio)\n",
    "    rank_dicts = dict(Counter(rank_values))\n",
    "    nb_classes = model.min_embedding.shape[0]\n",
    "    auc = compute_rank_roc(rank_dicts,nb_classes)\n",
    "    return top1, top10, top100, mean_rank, median_rank, per_rank, auc, rank_values\n",
    "\n",
    "def compute_rank_roc(ranks, n):\n",
    "    auc_lst = list(ranks.keys())\n",
    "    auc_x = auc_lst[1:]\n",
    "    auc_x.sort()\n",
    "    auc_y = []\n",
    "    tpr = 0\n",
    "    sum_rank = sum(ranks.values())\n",
    "    for x in auc_x:\n",
    "        tpr += ranks[x]\n",
    "        auc_y.append(tpr / sum_rank)\n",
    "    auc_x.append(n)\n",
    "    auc_y.append(1)\n",
    "    auc = np.trapz(auc_y, auc_x)/n\n",
    "    return auc\n",
    "\n",
    "def compute_accuracy(model, test_data):\n",
    "    nf1_min = model.min_embedding[test_data[:,[0,2]]]\n",
    "    nf1_delta = model.delta_embedding[test_data[:,[0,2]]]\n",
    "    nf1_max = nf1_min+torch.exp(nf1_delta)\n",
    "    boxes1 = Box(nf1_min[:, 0, :], nf1_max[:, 0, :])\n",
    "    boxes2 = Box(nf1_min[:, 1, :], nf1_max[:, 1, :])\n",
    "    probs = compute_cond_probs(model, boxes1, boxes2).cpu().detach().numpy()\n",
    "    return np.sum(probs==1)/probs.shape[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_file = f'data/{dataset}/{dataset}_test.txt'\n",
    "test_data = load_valid_data(test_file,classes,relations)\n",
    "test_data = torch.Tensor(test_data).long().to(device)\n",
    "# compute_mean_rank(model, test_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 0.005031005031005031 0.0522990522990523 8040.507137007137 4898.0 23174.20000000002 0.8246767108571226\n"
     ]
    }
   ],
   "source": [
    "\n",
    "top1,top10, top100, mean_rank, median_rank, per_rank, auc,rank_values = compute_rank(model, test_data, 90)\n",
    "print(top1,top10, top100, mean_rank, median_rank, per_rank, auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-0be36a9ddc1c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mPATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./models/box_el_galen_50_seed2222.pt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtop1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtop10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean_rank\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmedian_rank\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mper_rank\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrank_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_rank\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m90\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtop10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean_rank\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmedian_rank\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mper_rank\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/anaconda3/envs/box/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    590\u001b[0m                     \u001b[0mopened_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_position\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 592\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    593\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_legacy_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/anaconda3/envs/box/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[1;32m    849\u001b[0m     \u001b[0munpickler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpickler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m     \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersistent_load\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpersistent_load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 851\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    852\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_loaded_sparse_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/anaconda3/envs/box/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mpersistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m    841\u001b[0m         \u001b[0mdata_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloaded_storages\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 843\u001b[0;31m             \u001b[0mload_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_maybe_decode_ascii\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    844\u001b[0m         \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloaded_storages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    845\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/anaconda3/envs/box/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload_tensor\u001b[0;34m(data_type, size, key, location)\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m         \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_storage_from_record\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 832\u001b[0;31m         \u001b[0mloaded_storages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrestore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    833\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpersistent_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaved_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/anaconda3/envs/box/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdefault_restore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_package_registry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/anaconda3/envs/box/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_cuda_deserialize\u001b[0;34m(obj, location)\u001b[0m\n\u001b[1;32m    155\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mstorage_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspace/anaconda3/envs/box/lib/python3.6/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36m_cuda\u001b[0;34m(self, device, non_blocking, **kwargs)\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0mnew_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnew_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered"
     ]
    }
   ],
   "source": [
    "\n",
    "PATH = './models/box_el_galen_50_seed2222.pt'\n",
    "model = torch.load(PATH)\n",
    "\n",
    "top1,top10, top100, mean_rank, median_rank, per_rank, auc,rank_values = compute_rank(model, test_data, 90)\n",
    "print(top1,top10, top100, mean_rank, median_rank, per_rank, auc)\n",
    "print(compute_accuracy(model, test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "box",
   "language": "python",
   "name": "box"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
